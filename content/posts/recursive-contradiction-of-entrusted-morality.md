# The Recursive Contradiction of Entrusted Morality

A WSJ article profiles "the one woman Anthropic trusts to teach AI morals." Elon Musk attacks her fitness for the role. Others argue she's more trustworthy than Musk. Everyone debates *who* should hold the authority. Nobody questions whether the delegation itself is coherent.

## The structural problem the moment you entrust someone to define morality, you've separated moral reasoning from the individuals who need to exercise it. The entrusted party now optimizes for maintaining the framework — their role, their authority, their codified system — which is structurally different from the function morality actually serves. This isn't about the person selected. Put the wisest person alive in that seat and the structure produces the same drift. The corruption is in the act of entrusting, not in the character of the entrusted.

But there's a deeper contradiction. By stating who we should trust, we implicitly trust our own judgment in selecting who to trust — while claiming we need someone else's judgment for what to trust. The act of choosing who has good judgment *demonstrates* the very capacity we're claiming we lack.

And it's not even a simplification. Judging *what* to trust is local, contextual, revisable — you encounter a claim, evaluate it, update. The feedback loop is tight. Judging *who* to trust requires modeling an entire agent across contexts you can't observe. People trade a continuous series of tractable judgments for a single intractable one, and experience it as ease.

Worse, each act of delegation atrophies the judgment that made the selection possible. The more you outsource, the less capable you become of evaluating whether the outsourcing is working. The endpoint: people defending authorities on the basis of an original selection made by a version of themselves that no longer exists.

The question was never *who* should we entrust with defining morality. It's whether entrusting is a coherent operation at all.
