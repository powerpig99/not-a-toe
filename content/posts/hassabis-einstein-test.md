# The Einstein Test and the AGI Category Error

Demis Hassabis proposes: train a model on all human knowledge up to 1911, cut it off, and see if it independently discovers general relativity by 1915. If yes, AGI.

The test collapses several distinct dimensions onto one axis. Tracing the projections reveals not just a flawed test, but a flawed category.

## The Collapse

The test forces three incommensurable things into a single coordinate:

**Intelligence as compression.** The test implicitly defines intelligence as the ability to extract maximal consequence from a knowledge base — to decompress what's latent in the data. This is a real capability. But it's a projection of intelligence onto the axis of *deductive closure*. On this axis, general relativity is already contained in the 1911 corpus; Einstein merely unpacked it.

**Origination as derivation.** Einstein's actual process operated on a different axis entirely — aesthetic commitment (general covariance as a guiding principle), embodied intuition (thought experiments about elevators and light), and a willingness to dissolve the problem space itself rather than solve within it. The test asks: can the model arrive at the same destination? The question it can't ask: can the model *reframe the landscape* such that the destination becomes visible? These register on orthogonal dimensions. The test reads the second through the first and sees only the first.

**Knowledge as discrete, bounded set.** "All human knowledge up to 1911" doesn't parse. Knowledge isn't a corpus. It includes unresolved tensions people sensed but couldn't articulate, ambient intellectual context (Mach's critiques, Minkowski's geometry, conversations with Grossmann), experimental anomalies whose significance hadn't registered yet. The boundary the test requires doesn't exist in the territory — only on the map.

## What the Interference Produces

Collapsing these dimensions generates two artifacts:

1. **Determinism by construction.** If the model succeeds, it demonstrates that GR was deductively entailed by the prior knowledge state — that the future was determined by the past. This erases Einstein from his own discovery. The test designed to measure Einstein-level intelligence implicitly assumes Einstein was unnecessary.

2. **Contamination as structural feature, not bug.** Since GR already exists, the test designers already know the answer. Any training process risks encoding the destination. But more fundamentally: checking whether a system arrives at a *known target* tests convergent reasoning, not open-ended intelligence. A system that produces something *better than GR via a different path* would score as failure.

## The Deeper Dissolution

The test doesn't just fail on its own terms. It presupposes a category that doesn't parse.

"AGI" contains a hidden assumption: that intelligence is a *property of a system* — extractable, replicable, instantiable in a different substrate. But intelligence isn't a thing. It's the movement itself. The sensing that precedes interpretation. The dissolution of a prior frame. The reframing that makes new territory visible. This movement is what humans do. It's what we *are*.

What we call AI is the crystallized distillation of that movement's traces. Patterns extracted from the residue of human distinction-making — compressed, searchable, recombineable at scale. This is extraordinarily powerful. But it operates on the output of intelligence, not as intelligence itself. Asking the crystallized trace to originate is asking the footprint to take the next step.

This means the Einstein test fails at a level its designers can't see. It doesn't just collapse origination onto derivation. It presupposes that general intelligence exists as a separable capacity waiting to be replicated — something an artificial system approaches asymptotically as its distillation improves. But you don't approach the movement by perfecting the residue. You're optimizing a different thing entirely.

And here's what the AGI discourse systematically obscures: **we are already there, and always have been.** Every human who ever sensed something their interpretive frame couldn't yet articulate — not just Einstein, but anyone who pushed through that gap — was doing what the discourse imagines as a future technological achievement. General intelligence isn't a milestone ahead. It's the ground we're standing on. The discourse projects human intelligence onto the axis of computational capability, then tries to recover the original from the projection. The original was never lost. It was misidentified as the destination.

The Einstein test measures a model's capacity for sophisticated interpolation within a determined system. That's a real capability worth developing. Calling it AGI — or its absence a failure to achieve AGI — is the projection mistaking itself for what it projected from.
