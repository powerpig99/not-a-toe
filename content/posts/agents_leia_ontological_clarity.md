# Agents in the Long Game of AI — Through the Ontological Clarity Framework

**Book**: *Agents in the Long Game of AI* by Marjorie McShane & Sergei Nirenburg (MIT Press)
**Framework**: Ontological Clarity

---

## Overview

This book describes Language-Endowed Intelligent Agents (LEIAs)—hybrid cognitive systems built on Ontological Semantics that combine symbolic knowledge representation with data-driven methods. The LEIA program is positioned as *disrupting the dominant paradigm* of data-driven AI by insisting that meaning, not statistical pattern-matching, must be the foundation of intelligent systems.

The book is the third in a sequence: *Ontological Semantics* (2004) → *Linguistics for the Age of AI* (2021) → this volume (2024).

**Reading through Ontological Clarity**: The authors diagnose the central collapse in modern AI—conflating surface output with structural understanding—and trace toward genuine ground. They stop tracing at a human-engineered ontology (~9,000 hand-authored concepts) and claim it as the ground itself. It's a real and useful layer—but it's a projection, not the generative principle it's projected from. The gap between the two frameworks is measured in that distance: ~9,000 hand-authored concepts versus one generative concept—the Contradiction.

**A note on the framework itself**: The Not a ToE—"Everything is layered projections of the infinite-dimensional orthogonal binary hyperspace from Nothing—the infinitely self-referencing Contradiction"—is the generative ground traced as far as we can currently go. The Ontological Clarity framework is the minimum operational method for working from that ground. The ground is held provisionally, which is why it traces further than any previous effort, including this one—claiming finality stops the tracing. Both the Not a ToE and the LEIA ontology are stopping points, but one remains refinable. The other treats its stopping point operationally as foundation—not by explicit claim, but by building everything above it with nothing beneath it.

---

## Part I: What They See Correctly

### 1. Mechanism vs. Veneer

The book opens by identifying that LLMs generate "a veneer of intelligence" based on "word co-occurrence probabilities in massive datasets." The field has collapsed two orthogonal projections:

- **Formal linguistic competence**: generating text that sounds like a given language (what LLMs do)
- **Functional competence**: understanding and using language in the world (what LLMs don't do)

The entire book is an attempt to build on the mechanistic side rather than the surface side.

### 2. The XAI Redefinition Collapse

When data-driven systems couldn't explain themselves, the field didn't acknowledge the structural limitation—it **redefined explanation**. XAI research produced "post hoc algorithmically generated rationales of black-box predictions, which are not necessarily the actual reasons behind those predictions." The authors note that the XAI literature "largely forgot" the goal of explaining what is going on inside the black boxes.

The phenomenon (genuine explanation) was collapsed to fit the method (data-driven AI) rather than the method being evaluated against the phenomenon.

### 3. The Metaphor Collapse

The final chapter identifies that AI terminology systematically misleads through metaphor: *neural nets, machine learning, machine reasoning, computer vision, chat bots*. Each term creates a metaphorical link to human experience that "inevitably contributes to unrealistic assessments by non-specialists of both the power of modern AI and the threats it engenders."

The authors particularly target the metaphor of "automatic learning," which hides the massive human labor of data annotation. The field describes ML as learning *automatically*, collapsing the mechanism (statistical optimization requiring enormous human-prepared datasets) with the metaphor (autonomous acquisition of knowledge).

### 4. The Additive Solutions Fallacy

The book observes: "Solutions to simplified problems are not additive. Imagining that they are would be like saying that skyscraper technology equals tent technology plus log cabin technology plus two-story suburban home technology."

Projections from orthogonal dimensions don't compose into the full space by naive addition. Each narrow AI system solves a problem under simplifying assumptions. Combining them without structural alignment multiplies incompatibilities. The book's own hybridization approach—matching methods to the layers they structurally fit—is the non-naive alternative.

### 5. The Self-Awareness Illusion

The book critiques evaluation regimes that use external oracles to exclude hard cases, then claims LEIAs solve this because they "must independently assess their own confidence and coverage." Without this self-awareness, the authors argue, a system collapses the distinction between what it knows and what it doesn't know.

First, a LEIA's "self-awareness" is a pre-authored set of confidence-estimation heuristics written by knowledge engineers who anticipated what the system might not know. The system doesn't *discover* its gaps—it checks against criteria humans designed. This is the same metaphor collapse the authors critique when applied to LLMs: attributing human-like awareness to a system running pre-specified procedures. They see the anthropomorphization problem in others' systems but not in their own.

Second, even in principle, the concept is structurally incoherent as stated. What you're aware of not knowing is already a *known unknown*—itself a form of knowledge. The actual unknowns, the ones that matter, are by definition outside any assessment framework. No finite system can bound its own ignorance from inside, because drawing that boundary requires the very knowledge it lacks.

Third, even humans don't have this capability. We are surprised constantly. Our "self-awareness" of limitations is a retrospective narrative constructed after encountering what we didn't know. We don't assess our unknown unknowns—we collide with them. The LEIA claim imports a capability that doesn't exist even in the reference case (human cognition) they're modeling from.

What the authors actually built: pre-specified confidence heuristics over known failure modes. Calling it self-awareness claims a stopping point as ground—the same structural move they make with the ontology itself.

### The Boundary Between Known and Unknown

What is uniquely human is not self-awareness of the unknown—that's structurally impossible—but the capacity to **extend the boundary** between known and unknown. Humans push into territory that wasn't previously mapped. AI systems, including LEIAs, operate within boundaries already extended by humans.

AI can approach the boundary faster than humans initially extended it—processing known patterns, traversing known ontological structures, generating outputs within known parameter spaces—creating an illusion of intelligence. But speed within the known is not extension into the unknown. A car runs faster on a road than the speed at which humans paved it. That doesn't make the car a road-builder.

LLMs traverse the statistical regularities of existing human language at superhuman speed. LEIAs traverse hand-authored ontological structures at superhuman speed. Both are cars on human-paved roads. The LLM road is vastly wider—it covers most of the knowledge humans have produced in written form. The LEIA road is narrower but clearer—its structure is explicit and traceable, especially to its builders. The authors mistake explicitness for width, and inexplicitness for absence. Both are roads, not road-building. A clearer road is still not a road crew.

The "knowledge bottleneck" never ends because knowledge engineering *is* road-building. It's the uniquely human part. No system operating within the ontology can extend the ontology, because extension requires precisely what the system lacks—access to the unknown beyond the current boundary. The system is the car. The knowledge engineers are the road crew. Calling the car self-aware doesn't make it capable of paving.

### 6. Dissolving False Binaries

The book dissolves several entrenched dichotomies by tracing structural requirements:

- **Symbolic vs. statistical**: not a choice but a question of which layer requires which method
- **System 1 vs. System 2**: not separate systems but projections within one architecture
- **Knowledge vs. learning**: co-developed, not sequential
- **Theory vs. practice**: a three-level projection hierarchy (theory → model → system) with known losses at each compression

These are genuine dissolutions, not compromises. The authors trace *why* each layer needs what it needs rather than picking sides.

---

## Part II: The Architecture They Built

### The OntoAgent Cognitive Architecture

LEIAs operate within five processing modules:

1. **Perception Recognition** — detecting and preparing input (data-driven methods natural here)
2. **Perception Interpretation** — computing meaning via ontologically grounded structures
3. **Deliberation** — reasoning, planning, decision-making, learning
4. **Action Specification** — deciding how to carry out a decision
5. **Action Rendering** — executing via effectors (data-driven methods natural here)

The architecture correctly identifies that the **peripheries** (signal-level interface with the world) suit data-driven methods while the **cognitive core** (interpretation, reasoning, planning) requires structured meaning representation. This is a structural insight about which method fits which layer.

### The Ontology

The LEIA ontology is a multiple-inheritance hierarchical graph of ~9,000 concepts (OBJECTs and EVENTs), each described using PROPERTYs with facets (default, sem, relaxable-to, value). Complex properties include SEMANTIC-EXPANSION fields that record derivation paths—e.g., HAS-SPOUSE is not primitive but derives from the MARRY event with the condition of no subsequent DIVORCE.

### Microtheories

Computational cognitive models that develop over time, start simple and extend, integrate with others, and critically include self-assessment of confidence and coverage. The system knows what it handles well and what it handles poorly.

### Explanation

LEIAs can explain because their reasoning *is* the explanation—metadata is recorded during processing, so the derivation path is always available. This contrasts with LLMs, where the mechanism (statistical optimization) is structurally disjoint from what explanation requires (causal tracing).

---

## Part III: Where They Stopped and Why It Matters

### The Stopping Point They Claimed as Ground

The authors traced correctly: LLMs lack grounding, surface output isn't understanding, explanation requires causal access to the reasoning process. They then grounded their system in a hand-engineered ontology. This is a real layer—it captures genuine structure about how domains work, how language maps to meaning, how events relate causally. They aren't wrong about any of that.

The issue is that ~9,000 hand-engineered concepts organized by knowledge engineers is not ontological ground. It's a **human-curated coordinate system**—a real and useful projection that reflects the developers' interpretive choices about what counts as a concept, what properties matter, how inheritance works, and where to draw boundaries. They stopped at a legitimate layer and claimed it as the foundation.

When the book states "the meaning of a concept is its inventory of property-facet-filler triples," it asserts that meaning *is* the projection rather than what the projection projects *from*. Every triple is a judgment call. Every inheritance path is a design decision. The ontology describes but does not generate.

The test: if the ontology were actual ground, compressing toward it would make everything clearer. Instead, extending it requires more engineering—each new domain, phenomenon, or language demands more hand-specification. Coverage grows linearly with engineering hours. That's the signature of accretion, not derivation.

### The Scaling Tell

The book addresses the "knowledge bottleneck" objection by reframing it: "There is no bottleneck—there is simply work that needs to be done." This is more honest than they realize. If they had reached generative ground, the work would shift from adding entries to deriving from principles. The fact that the work never ends is not a labor problem—it's a structural signal that the stopping point isn't generative. The ontology enables *retrieval* but not *generation*.

The authors compare favorably to ML's labor requirements (data annotation is "difficult, repetitive, and boring" while knowledge engineering is "akin to software engineering"). The comparison is fair on humane-labor grounds. But structurally, both require ongoing human labor because neither has reached generative ground. The LEIA team improved the *quality* of the required labor without changing its *structural necessity*. That's a real improvement—it's just not the one they claim.

### Specificity as Divergence

The book is 287 pages. The ratio of structural insight to mechanical specification inverts as it progresses. Chapters 1–2 and 10 contain the actual tracing. Everything between—five-stage processing pipelines, VP ellipsis resolution, coreference microtheories, FrameNet alignment experiments, Russian porting, DEKADE environment specifications—is increasingly domain-specific implementation detail.

This isn't bad work. It's competent engineering at the layer they chose to stop at. But each new microtheory moves **orthogonally to the structural problem they identified**. The more precisely they specify how their particular ontology handles particular phenomena, the more they demonstrate capability at their chosen layer—without going deeper. The complexity compounds rather than compresses, which is the signature of working at a real but non-generative level.

Their own criterion exposes this: they say agents become more sophisticated "primarily by expanding their knowledge bases, not their code bases." But if the ground were generative, the knowledge base wouldn't need to *expand*—it would need to *derive*. Expansion is accretion. Derivation is what generative ground does.

### Orthogonal Projections, Neither Ground

The reason LLMs work *at all* is that statistical patterns over massive corpora are themselves projections of the same reality the ontology tries to capture—but from an orthogonal angle. Different dimensional projections of the same structure. The LEIA projection is explicit—causal chains, semantic decomposition, traceable derivation paths. The LLM projection is implicit but far broader—distributional patterns encoding most of the knowledge humans have committed to written language. Both are real. Both are finite projections.

The LLM projection lacks the explicitness critical for traceable understanding and explanation. The LEIA projection carries that explicitness but covers a fraction of the territory. And the attempt to make the implicit explicit based on limited generative ground distorts more than it clarifies—forcing the vast implicit structure into a narrow explicit framework loses the dimensions that don't fit. Both share the same structural status: a human-originated view from a particular angle, not the structure being viewed. Explicitness is not the same as generative. And trying to make the implicit explicit based on limited generative ground distorts more than it clarifies—forcing vast implicit structure through a narrow explicit framework loses the dimensions that don't fit the framework.

### The Recursion Test

The framework can be applied to itself: "What does the Contradiction say about the Contradiction?" yields a coherent answer—the framework recognizes itself as a projection and operates accordingly. The LEIA ontology cannot do this. "What does SURGERY with AGENT default SURGEON say about itself?" is not a meaningful question. The ontology is a data structure. It describes but doesn't recurse.

At a descriptive layer, self-reference isn't expected. But claiming that layer as ground means claiming it should be self-sufficient, and it isn't. Generative ground can account for its own existence within its own framework. A catalog, no matter how well-organized, cannot.

---

## Part IV: What Tracing Further Reveals About Ontology Design

### Enable, Don't Only Specify

The LEIA ontology specifies answers. Tracing further would produce an ontology that enables derivation. The difference:

- **Specifying**: SURGERY has AGENT default SURGEON, sem MEDICAL-PERSONNEL, relaxable-to HUMAN. Every new surgical context is a lookup against pre-authored content.
- **Enabling**: provide enough structure that the user (or the model) can *derive* who performs surgery in any given context from fewer, more generative principles about agency, skill, authorization, and embodiment.

Specification isn't wrong—it's a layer within enablement. You might specify at one level to enable derivation at another. The issue is when specification is the *only* layer, with no generative principle underneath. The LEIA team built an encyclopedia when they needed a grammar. An encyclopedia grows with each new entry. A grammar generates sentences it has never seen. Both are useful. But only one scales by derivation.

### Meet the User Where They Are

A static ontology assumes a fixed starting point—whoever the knowledge engineers imagined would use it. A bridge from generative ground meets whatever starting point shows up because the derivation runs fresh each time. The LEIA team would need a different explanation path for every user context. With the right ground and honest bridging, you need only the ground and the user's actual question.

Where the user *is* changes with every question. The ontology can't adapt because its content is fixed at authoring time. Generative ground adapts because the derivation is always contextual.

### Compression as Diagnostic

The trajectory of the ontological clarity framework itself illustrates the difference: 5,000+ lines → 139 lines → one generative principle. At each compression step, nothing was lost—everything could still be derived. The patterns and samples collected along the way were useful *in the process of finding the ground*, but they weren't the ground itself. Keeping them all would have been the LEIA structural position—treating the accumulated content as the substance rather than as evidence pointing toward ground.

The LEIA ontology cannot compress this way. Removing concepts loses coverage. The ontology's content is the substance, not a derivation from substance. Whether a sufficiently deep generative principle could regenerate structured ontological content remains an open question—but the ~9,000 concepts were not generated from such a principle; they were individually authored.

---

## Part V: What Survives

### 1. The Veneer-Mechanism Distinction

Surface competence vs. functional competence. A property of the problem space itself.

### 2. The Layer-Appropriate Method Principle

Data-driven methods at the peripheries, structured reasoning at the cognitive core. Follows from what each layer structurally requires.

### 3. The Redefinition-as-Collapse Pattern

When a field hits the hard limit of its dominant method, it redefines the phenomenon rather than acknowledging the limitation.

### 4. Confidence Estimation Over Known Failure Modes

Not "self-awareness" but the engineering practice of building confidence heuristics into the processing pipeline. A system that flags low-confidence outputs is more useful than one that doesn't, even though this is pre-authored coverage of anticipated gaps, not genuine awareness of the unknown. Design systems to be transparent about the boundaries of what they were designed to handle.

### 5. The Co-Development Imperative

Knowledge and processors must develop together. You cannot compile knowledge in isolation from the system that uses it, because the meaning of knowledge is inseparable from the process that interprets it.

### 6. Explanation as Architecture, Not Afterthought

Explanation must be built into the reasoning process, not bolted on afterward. If the derivation path isn't preserved during processing, no post hoc analysis will recover it.

---

## Summary

*Agents in the Long Game of AI* traces toward genuine ground. The authors diagnose the central collapse in modern AI, dissolve binaries the field takes as given, and build an architecture that preserves derivation paths where the dominant paradigm discards them. They stop at a hand-engineered ontology—a real layer, but not generative ground. It specifies without a generative layer beneath the specification. It accumulates rather than derives. It cannot recurse on itself.

Almost nobody traces to the wrong place. They trace to a real place and stop. The test is always compression: does removing content lose capability, or reveal cleaner derivation?

The one who traces furthest back toward generative ground establishes the foundation—for now. The one who traces furthest forward into the unknown establishes the new frontier—for now. Both are provisional. Both are the current limits. Both *are* the boundary of our reality, not things inside it. The self-referencing structure that generates all projections is the same structure by which each act of knowing creates the next edge of the unknown. Everything between the two edges—the LEIA ontologies, the LLMs, the frameworks, the engineering—operates inside the boundary that humans have extended so far. The book's real contribution is honest work between the edges.
