## Why AI Is Exposing the Fatal Flaw in Modern Schooling

A viral post recently praised Alpha School in San Francisco: AI-taught kids outperforming the top 0.1% globally, five-year-olds performing at the level of nine-year-olds, teenagers operating at college level and beyond. Impressive numbers. But any measurement that ignores pre-enrollment baselines and reports only outcomes is deeply misleading. A $30k–$50k/year private school attracting high-agency, high-SES families starts with kids already in the top tail. Without rigorous pre/post controls, the gains tell you almost nothing about the method.

That was my initial reaction. But the deeper problem hit harder: if AI is truly the transformative ingredient, then wrapping it in a physical school—tuition, campus, mandatory hours—starts to look incoherent. The logical endpoint of "AI personalizes and accelerates learning" is that the same results are achievable independently, with far less time and near-zero cost for core academics. If the tool works, the wrapper is overhead. If it doesn't work without the wrapper, the tool isn't what's working.

My own experience confirms this. My kids learn far more in roughly one hour per day with MathAcademy—an adaptive mastery platform—than they ever did in a full school day, without even fully leveraging frontier models like Claude or GPT-4o for deeper explanations, proofs, or custom problems. Years of curriculum compressed into months. Genuine understanding instead of compliance. Plenty of time left for projects, play, real life. My own AI-mediated productivity as an adult follows the same pattern. The gap is not incremental. It's structural. And this isn't just anecdotal. MathAcademy, Khanmigo, Synthesis, or raw prompting of frontier models are already enabling self-directed learners to hit elite benchmarks with minimal daily input. Alpha's model can be replicated asynchronously without the institutional layer. The more powerful AI becomes, the harder it is to justify the overhead.

Schools defend their role with structured accountability, social and emotional development, curated curriculum, and credentialing. These are real functions. But notice: they're bundled. Knowledge transmission, social coordination, childcare logistics, and credential signaling are separate problems forced through a single institution. AI unbundles the first. The others don't require a school—they require coordination, which is a different and increasingly solvable problem.

---

But the deeper issue isn't efficiency. It's what happens to the connection between action and result.

When you act and the consequence flows directly from your action, you learn. Not because someone told you to, but because reality responded. That tight loop—choice, action, feedback—is how competence and judgment actually develop. This has always been true. It's how learning worked before schools existed. It's how every skill you actually retained was built. The mechanism was never missing. It was buried.

Mass schooling systematically buries it. Your effort is filtered through grading rubrics, pacing constraints, group averages, and institutional logic. The feedback you receive reflects the system's priorities more than your own actions. The connection between what you do and what happens back becomes noisy, delayed, indirect.

This creates a dependency cycle. Loosened feedback loops produce passivity—not because learners are deficient, but because the environment stopped responding to them. The system then points to this passivity as evidence that more structure is needed. More scaffolding, more pacing control, more institutional mediation. The institution's survival metrics—enrollment, retention, funding—scale with perceived necessity. Necessity grows when its outputs lack self-sufficiency. This isn't conspiracy. It's incentive structure operating as designed.

---

AI didn't invent tight feedback loops. It amplified the signal past the noise floor.

Previously, preserving direct action-result coupling in education required exceptional effort at the edge. Parents who could see through institutional noise. Rare teachers who got out of the way. Autodidacts who built their own paths. The signal was always there; extracting it required unusual filtering capacity.

What AI does is raise the signal-to-noise ratio at the platform level. You ask, you get a response calibrated to your actual understanding. You push deeper, it goes deeper. You skip what you know, you dwell where you're stuck. The scaffolding responds to you and naturally fades as competence rises. The mechanism that was always operating is now visible without requiring exceptional effort to uncover it.

---

This reveals something important about where AI is actually heading—and why this time the incentives are different.

Traditional platforms capture value through specificity. Specific workflows, specific content, specific user patterns. Every function absorbed becomes a dependency monetized. The platform inserts itself between your action and your result, and that intermediation becomes the product. This is the same dynamic as institutional schooling: the more the platform mediates, the more it can extract, the more it needs you to depend on its mediation.

AI inverts this. The platform becomes more valuable by becoming more general. Claude doesn't improve by building a specific curriculum for my kids. It improves by becoming better at reasoning, adapting, responding—general capabilities that empower personalization at the edge. The personalization is mine. The platform never needed to touch it.

This is a fundamentally different incentive geometry. Traditional platforms loosen action-result coupling by design—inserting institutional logic between you and your feedback is how they capture value. AI platforms tighten that coupling by design—because tighter coupling means more usage, more diverse usage, more demand for general capability. The incentives align instead of conflict. The better the general tool becomes, the more it empowers individual adaptation, the less it needs to intermediate.

My own interaction with AI—including the conversation that refined this very essay—is direct evidence. No curriculum. No institutional layer. Just general capability meeting edge context. The result is something no pre-designed program could have produced, because the adaptation happened at the edge, in real time, shaped by my own judgment and direction.

---

This also shifts who can access it, and how the expansion works.

Previously, preserving tight feedback loops required the capacity to filter—to see through institutional noise to the underlying mechanism. That's hard, rare, and correlated with resources. It's why the opt-out path has historically served a narrow cohort.

As the signal-to-noise ratio improves at the platform level, the edge requirement drops. From "exceptional filter" to "don't add noise back in." That's a much larger population. The expansion path isn't "everyone becomes a curriculum designer." It's that the signal becomes clear enough that less and less filtering is required to act on it.

An honest constraint remains: tight feedback loops are only as useful as your ability to read the results. A young child working with AI still needs someone who can evaluate whether the learning is real—whether "I asked and got an answer" constitutes understanding or just its feeling. In our family, the parent does this work. That's not trivial.

But notice: this is the same evaluation work that was always needed. Schools didn't solve it—they obscured it behind grades and credentials that substituted institutional judgment for direct assessment. AI at least makes the question visible again. And as general AI capability improves, even this evaluation support becomes increasingly accessible—not by replacing parental judgment, but by making the signal legible enough that less interpretation is required.

---

For families navigating this now: the downstream frictions—college admissions, socialization, credentials—don't bind when competence develops through direct feedback rather than institutional dependency. Those gates become optional, engaged when they serve intrinsic goals. Bypass via demonstrated capability, apprenticeships, or direct contribution. When the foundation is real coupling between action and result, credentials become something you choose, not something you need.

This shift is accelerating. As AI raises the signal-to-noise ratio, the cohort of effective opt-outs grows. Each one reveals institutional necessities as partly artifacts of the institution's own feedback degradation.

The future isn't better schools. It's the mechanism that was always there—direct coupling between action and result—finally amplified past the noise that obscured it. Schools loosened it. AI is restoring it. The restoration is becoming accessible to expanding circles.

For families who can see it: build the coupling directly, let results speak.

The dependency model can't survive the asymmetry forever.
